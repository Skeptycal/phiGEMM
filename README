Copyright (C) 2011-2014 Quantum ESPRESSO Foundation
Copyright (C) 2010-2011 Irish Centre for High-End Computing (ICHEC)

All the material included in this distribution is free software;
you can redistribute it and/or modify it under the terms of the GNU
General Public License as published by the Free Software Foundation;
either version 2 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License
for more details.

You should have received a copy of the GNU General Public License along
with this program; if not, write to the Free Software Foundation, Inc.,
675 Mass Ave, Cambridge, MA 02139, USA.



>>> phiGEMM: CPU-GPU hybrid matrix-matrix multiplication library <<<

Authors & Maintainers:
  Filippo Spiga, spiga [-DOT-] filippo [-AT-] gmail [-DOT-] com
                 filippo [-DOT-] spiga [-AT] quantum-espresso [-DOT-] org
  Ivan Girotto, ivan [-DOT-] girotto [-AT-] ictp [-DOT-] it
  
Past contributors:  
  Philip Yang, phi [-AT-] cs [-DOT-] umd [-DOT-] edu


Starting from a previous NVIDIA project*, ICHEC has developed a library
called phiGEMM within the PRACE project. This library allows the target 
application to perform [S/D/C/Z]GEMM operations using both CPU and GPU 
simultaneously. 

     C := alpha*op( A )*op( B ) + beta*C,

  where  op( X ) is one of

     op( X ) = X or  op( X ) = X**T  or  op( X ) = X**H,

  alpha and beta are scalars, and A, B and C are matrices, with op( A )
  an m by k matrix,  op( B )  a  k by n matrix and  C an m by n matrix.

This library implements both the data transfer management 
and the split logic (amount of computation to be performed in parallel on 
each device, CPU and GPU, in order to achieve an optimal load balance). 
Since phiGEMM follows standard BLAS interfaces, the users just need to 
link to the library, like any other common library such as MKL, ACML,
GotoBLAS, etc. The phiGEMM matrix-matrix multiplication calls 
simultaneously an external BLAS library single- or multi-threaded) for 
the CPU-side and one CUBLAS kernel instance on the GPU, as efficiently 
as possible.


If you are planning to include phiGEMM in your code and use it,
We shall greatly appreciate if scientific work done using phiGEMM will 
contain an explicit acknowledgment and the following references :

* F. Spiga and I. Girotto, "phiGEMM: a CPU-GPU library for porting Quantum 
ESPRESSO on hybrid systems", Proceeding of 20th Euromicro International 
Conference on Parallel, Distributed and Network-Based Computing (PDP2012), 
Special Session on GPU Computing and Hybrid Computing, IEEE Computer Society,
(ISBN 978-0-7695-4633-9), pp. 368-375 (2012)

* M. Fatica, "Accelerating LINPACK with CUDA on heterogeneous clusters." 
GPGPU-2: Proceedings of 2nd Workshop on General Purpose Processing on 
Graphics Processing Units (New York, NY, USA), ACM, 2009, pp. 46--51.


0. >>> Know issues and bugs [READ THIS FIRST]

- SPECIAL-K and multi-threaded ACML (acml_mp) can produce some numerical
  errors (suggestion: keep SPECIAL-K disabled and test it before use it in
  production)
- MAGMABLAS interface not yet working
- if your code mixes BLAS/LAPACK/ScaLAPACK calls is better to replace
  GEMM symbols manually (or using the preprocessor) where it is necessary
  in order to avoid troubles or big performance loss
- call-by-call profiling works only for INTEL and GNU compiler
- some internal parameters are still hard-coded and manual code changes are 
  required to tune them


1. >>> Requirements

- NVIDIA cards
- CUDA SDK (>=4.0, >= 5.x)
- GCC/GFORTRAN
- Intel compilers (version >=11.x) or PGI compilers (version >=10.x)
- MKL (version >=10.x) or ACML (version >= 5.x)


2. >>> How to compile the library

Recognized options of the configure

--enable-parallel       : enable the MPI support in case of self-initialization
                          (default: no)
                          
--with-cuda-dir=<path>  : specify the path where CUDA is installed (mandatory)
--with-magma-dir=<path> : specify the path where MAGMABLAS is installed 
                          (not yet working)
                          
--enable-cpu-multithread: enable the multi-threading library on CPU side (MKL)
                          (default: yes)
--with-cpu-only         : everything is performed only by the CPU, useful for
                          call-by-call profiling (default: no)
--with-gpu-only         : everything is performed only by the GPU, useful for
                          performance comparison and validation (default: no)
                          

--enable-debug          : turn on verbose debug messages on the stdout
                          (default: no)
--enable-profiling      : turn on the phiGEMM call-per-call profile. File in
                          CSV format. (default: no)
--enable-pinned         : the library marks as non-pageable the host memory
                          (default: no) 
--with-special-k        : enable SPECIAL-K for bad-shaped rectangular matrices
                          (default: no)
--enable-multi-gpu      : enable multi-GPU support (default: no) 


IMPORTANT NOTE: the multi-GPU **REQUIRES** memory allocations at the host side 
                as non-pageable (pinned memory) otherwise all the H2D and D2H 
                transfers are serialized (bad performance!)

This is an example:

$ ./configure --with-cuda-dir=/ichec/packages/cuda/4.0/cuda 
    --enable-profiling

IMPORTANT NOTE: remember to have CUDA nvcc and CUDA libs properly set in the 
                environment

Then eventually edit the file "make.inc". Finally run "make".

The "libphigemm.a" and "libphigemm.so" will be placed under "./lib"


3. >>> the SPLIT factor

During the usage of the library, it is mandatory to specify through a 
environmental variable the SPLIT FACTOR. There are 3 recognized variables, 
one for each *GEMM routine implemented. This factor, a value strictly between 
0 and 1, tells to the library how much of the calculation has to be performed 
on the CPU and how much on the GPU. In detail, if PHI_DGEMM_SPLIT is 0.95 it 
means that the 95% of the computation will be performed by the GPU, the 5% by 
the CPU.
 
Here an example:
 
export PHI_SGEMM_SPLIT=0.85
export PHI_DGEMM_SPLIT=0.85
export PHI_CGEMM_SPLIT=0.9
export PHI_ZGEMM_SPLIT=0.9
 
The overall performances of the library, compared to the pure CUBLAS calls, are
strongly dependent by this split factor.
 
Only the test-suit is able to calculate the optimal split factor comparing 
the performance of the GPU and the CPU, varying the size and the type of the 
data (FLOAT, DOUBLE, SINGLE COMPLEX and COMPLEX).
 
 
The SPECIAL-K feature allows to combine GPU/CPU operations in a way to maximize
the GPU FLOP throughput performing all matrix-matrix operation on the GPU (the 
alpha*op(A)*op(B)) and by adding the final term on the CPU ( beta*C). If the 
library is compiled with SPECIAL-K enabled then the SPECIAL-K kernel is 
triggered when those conditions occur:

	
  // Matrices are small but not so small...
  if ( (n >= 64) && (m >= 64) ){
    
    // over the UPPER limit, they have to be rectangular...
	if ( ((n >= 1025) && (m >= 1025)) && 
	   ((ratio_km >= 20) || (ratio_kn >= 20)) )
	  return SPECIAL-K;
	
	// below the UPPER limit, they have to be very rectangular...
    if ( ((n < 256) && (m < 256)) && 
       ((ratio_km >= 40) || (ratio_kn >= 40)) )
	  return SPECIAL-K;
  }
	
where:
 
   ratio_km = (double) k/m;
   ratio_kn = (double) k/n;

SPECIAL-K is very effective as the ratio_km and/or ratio_kn are high. It
produces good performance boost especially if a non sub-optimal SPLIT FACTOR
is used.
                
IMPORTANT NOTE: This is an empirical tuning based on previous testing. 
                Any information provided to improve this heuristic is 
                really appreciated.

	
4. >>> Internal test-suite 

This package contains a small test-suite able to perform calculation using 
CUBLAS, MKL and phiGEMM comparing the results. The test-suite does not 
perform a check of the results, pleas use for single test "single_test.c".

In order to auto-tune the split factor using the best CPU and the best (pure) 
CPU performance, it is necessary to recompile the library with the configure 
option "--with-split-by-hand"

These hard-coded parameters inside the file "single_test.c" allow you
to test and compare a multi-threaded BLAS, a pure CUBLAS + data transfer and a 
phiGEMM call.

Performances are in GFlops. It is possible to specify additional flags, see 
"testing/Makefile" for additional details.

In order to compile the test-suite:
- first compile the phiGEMM library
- move to the "testing" directory
- "make"

The binary will be placed under "./bin". The executable requires these 
parameters

<nGPU> <m> <k> <n> <lower split-factor> <upper split-factor> <step>
        
for matrix multiplication C( m, n ) = A( m, k ) x B( k, n )


5. >>> How to use phiGEMM in your code

The usage of the phiGEMM library in your code is quite straightforward. The 
library itself has two "way" to work:
- CASE 1: for every call, phiGEMM allocates the proper amount of memory required 
to run the calculation. This is default.
- CASE 2: in order to speed-up the memory allocation on the GPU memory, it is 
possible to preallocate a fixed amount of memory on the GPU card and pass a 
pointer to phiGEMM. This memory can be larger than the one really needed. This 
is very useful because if you use phiGEMM in a real code, the dimensions of the
matrices involved in *GEMM operation can change.

At the beginning at and the end of your program, to initialize the phiGEMM
library (case 2) you have to add these lines:


    #define MAX_GPU 2
    typedef void* exampleMemDevPtr[MAX_GPU];
    typedef size_t exampleMemSizes[MAX_GPU];
    typedef int exampleTestDeviceIds[MAX_GPU];

    exampleTestMemDevPtr dev_scratch_QE;
    exampleTestMemSizes gpu_memory_to_allocate;
    exampleDeviceIds devicesToBond;

    /* Number of GPU considered */
    int nGPU = 1;

   // Allocate 1 GByte of GPU memory of GPU id 0
   devicesToBond[0] = 0;
   gpu_memory_to_allocate[0] = (size_t) 1000000000;
   
   ierr = cudaMalloc ( (void**) &dev_scratch_QE[0], \
                                    (size_t) gpu_memory_to_allocate[0] );
   if ( ierr != cudaSuccess) {
       fprintf( stderr, "Error in memory allocation (%d)!", ierr );
       exit(EXIT_FAILURE);
   }
           
    phiGemmInit( nGPU, (exampleMemDevPtr*)&dev_scratch_QE, \
          (exampleMemSizes *)&gpu_memory_to_allocate, (int *)&devicesToBond);


To "clean" the environment at the end of your program (strongly suggested!) 
simply  add before the final exit:
 
   ierr = cudaFree ( dev_scratch_QE[0] );
   phiGemmShutdown();


Let's assume that we want to use the phiGEMM library inside a file called 
"pippo.f90" or "pippo.c". In order to have a fine-grain control of which *GEMM 
operations will work using CPU+GPU, you can simply substitute the 
"{S/D/C/Z}GEMM" names with the "{phiS, phiD, phiC, phiZ}gemm" routine names. 

So for example this DGEMM call in C language:

   DGEMM(&ta, &tb, &m, &n, &k, &alpha, A, &lda, B, &ldb, &beta, C, &m);

will become:
 
   phiDgemm(&ta, &tb, &m, &n, &k, &alpha, A, &lda, B, &ldb, &beta, C, &m);


If you want to use phiGEMM for all the DGEMM in your file or in your C program 
you can easily add at the beginning of the files these lines:

  #define SGEMM phisgemm
  #define DGEMM phidgemm
  #define CGEMM phicgemm
  #define ZGEMM phizgemm

and at compile-time all the substitutions will be automatic.

Otherwise there is a FORTRAN 90 module called 'phigemm' that can be used 
in this way

  USE phigemm, ONLY : DGEMM => phidgemm, SGEMM => phisgemm, 
                      CGEMM => phicgemm, ZGEMM => phizgemm

IMPORTANT NOTE: the use the "profiling" advanced feature it is still necessary 
                to add few define at the beginning of each file where phiGEMM 
                is used in this way
                
#define _STRING_LINE_(s) #s
#define _STRING_LINE2_(s) _STRING_LINE_(s)
#define __LINESTR__ _STRING_LINE2_(__LINE__)
#define DGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC) DGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC,__FILE__,__LINESTR__)
#define SGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC) SGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC,__FILE__,__LINESTR__)
#define CGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC) CGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC,__FILE__,__LINESTR__)
#define ZGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC) ZGEMM(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC,__FILE__,__LINESTR__)

                These lines extend the prototype of each BLAS call in order to
                capture the exact file name and the exact line where every GEMM 
                is called at run-time.
                
IMPORTANT NOTE: be sure to cover both lover case (dgemm) and upper case (DGEMM).
                I strongly suggest to uniform the convention (or all lower or
                all upper case) before define the phiGEMM symbols.
                 
phiGEMM will provide a way to link directly the library without any change of 
the source code. However at the beginning it is better to keep a control about 
what, where and how a standard *GEMM operation is replaced with a phiGEMM ones.

IMPORTANT NOTE: this feature is not yet in the repository because can 
                potentially breaks the application. We are working to make it 
                100% reliable and safe.

                
7. >>> Automatic code instruction

Under the folder "script" there are two Python script that can help to made 
some automatic modifications of your code in order to exploit phiGEMM.

Run the command:
$ grep -rl -E 'gemm|GEMM' src/* | python addPhigemmSymbs.py

to identify all the files where a GEMM call is and then pass the obtained
list to the "addPhigemmSymbs.py" script. The script adds a header to each 
file where some preprocessor macros are defined. The scripts take care 
of full lower or  full upper capital GEMM symbols. The same script creates 
a ".orig" copy of each file that contains the original source code.

To restore the original source code simply run the command:
$ grep -rl -E 'gemm|GEMM' src/* | python remPhigemmSymbs.py 

These script are still in "beta". Please report any error or misfunction. 

 
8. >>> Releases

v2.0.0 - December 15, 2012  [ EXPECTED ]
v1.9.9 - October 1, 2012    ( various fixes at several levels )
v1.9   - May 23, 2012       ( fixes + compatibility CUDA 5.x )
v1.8   - April 1, 2012      ( ACML and PGI supported - internal tests work )
v1.7   - March 10, 2012     ( special DGEMM/ZGEMM for rectangular matrices )
v1.6   - January 19, 2012   ( various little improvements and fixes )
v1.5   - December 9, 2011   ( internal "pinning", experimental )
v1.4   - September 26, 2011 ( FORTRAN90 module module)
v1.3   - September 10, 2011 ( version optimized for both pinned and non-pinned )
v1.2   - August 29, 2011
v1.1   - July 12, 2011 
v1.0   - July 4, 2011       ( First official version )
v0.9   - July 2, 2011       ( Important fixes for multi-GPU )
v0.8   - June 29, 2011      ( Compatible with version for CUDA >=4.0 only! )
v0.7   - May 29, 2011
v0.6   - May 9, 2011
v0.5   - May 3, 2011
v0.4   - March 24, 2011
v0.3   - March 24, 2011
v0.2   - February 21, 2011
v0.1   - January 27, 2011


7. >>> Contacts

For any additional information or suggestion please contact 
- Filippo Spiga, spiga.filippo@gmail.com 
